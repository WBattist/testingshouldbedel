# evaluation.ipynb

import numpy as np
from utils.audio_processing import load_audio_data, preprocess_audio
from utils.evaluation import evaluate_model
from models.acoustic_model import build_acoustic_model

# Load configuration settings
config = {
    'data': {
        'test_samples_path': 'data/test_samples/',
        'sample_rate': 16000,
        'n_mfcc': 13
    },
    'model': {
        'acoustic_model_path': 'models/acoustic_model.h5'
    }
}

# Load and preprocess test data
test_data = load_audio_data(config['data']['test_samples_path'], config['data']['sample_rate'])
X_test, y_test = preprocess_audio(test_data, config['data']['n_mfcc'])

# Load the trained acoustic model
model = build_acoustic_model(input_shape=(None, config['data']['n_mfcc']))
model.load_weights(config['model']['acoustic_model_path'])

# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Loss: {loss}")
print(f"Test Accuracy: {accuracy}")

# Additional evaluation metrics, like Word Error Rate (WER)
from utils.evaluation import calculate_wer

# Assuming we have predictions and true labels
predictions = model.predict(X_test)
true_labels = y_test

# Calculate WER
wer = calculate_wer(true_labels, predictions)
print(f"Word Error Rate (WER): {wer}")

# Plot confusion matrix (if applicable)
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

# Assuming true_labels and predictions are available
cm = confusion_matrix(np.argmax(true_labels, axis=1), np.argmax(predictions, axis=1))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()
